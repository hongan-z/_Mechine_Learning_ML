# -*- coding: utf-8 -*-
"""naiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19P20wEOL9toJovqtMFf-rA7rOT_4GXPh

# Naive Bayes Classification using Scikit-learn
Source: https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn

## Naive Bayes Classifier

### Defining Dataset
In this example, you can use the dummy dataset with three columns: weather, temperature, and play. The first two are features (weather, temperature) and the other is the label.
"""

# Assigning features and label variables
weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',
'Rainy','Sunny','Overcast','Overcast','Rainy']
temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']

# play=['No','No','No','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','No','No']
play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']

"""### Encoding Features
First, you need to convert these string labels into numbers. for example: 'Overcast', 'Rainy', 'Sunny' as 0, 1, 2. This is known as label encoding. Scikit-learn provides LabelEncoder library for encoding labels with a value between 0 and one less than the number of discrete classes.
"""

# Import LabelEncoder
from sklearn import preprocessing
#creating labelEncoder
le = preprocessing.LabelEncoder()
# Converting string labels into numbers.
weather_encoded=le.fit_transform(weather)
print(weather_encoded)
# 0: overcast, 1: rainy, 2: sunny

"""Similarly, you can also encode temp and play columns."""

# Converting string labels into numbers
temp_encoded=le.fit_transform(temp)
label=le.fit_transform(play)

print("Temp:",temp_encoded)
# 0: cool, 1: hot, 2: mild 

print("Play:",label)

"""Now combine both the features (weather and temp) in a single variable (list of tuples).


"""

#Combinig weather and temp into single listof tuples
import numpy as np

# features = list(zip(weather_encoded,temp_encoded))
features=np.append([weather_encoded],[temp_encoded],axis=0)
features = features.T
print(features)

import pandas as pd
import numpy as np

data = pd.DataFrame({'weather':weather_encoded,'temperature':temp_encoded,'play':label})
data

"""### Naive Bayes

Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Bayes’ theorem states the following relationship, given class variable $y$ and dependent feature vector $x(1)$ through $x(d)$:

\begin{align*}
P(y | [x(1),\ldots, x(d)]) &\propto P([x(1),\ldots, x(d)] | y) P(y) \\
& = P(x(1)|y) \ldots P(x(d)|y) P(y) \\
& = P(y) \prod_{k=1}^d P(x(k)|y)
\end{align*}

Let's compute the individual $P(x(k)|y=0), P(x(k)|y=1)$ for each $k$ and the prior probability $P(y)$

In our example, k = 1,2
"""

# prior probablity of label 
# P(y) = ? 

y = label

# count the instances 
Py = np.zeros(2)
for i in range(0,len(y)):   
  Py[y[i]] += 1

# normalize each column by total number to convert counts into probability 
Py = Py/np.sum(Py,0)

# P(y) 
data = pd.DataFrame(Py,columns=['P(y)'],index=['y=0','y=1'])
data

# k = 1: weather feature 
# P(x(1) | y) = ? 

X = weather_encoded
y = label

# count the instances 
Px1y = np.zeros([3,2])
for i in range(0,len(X)):   
  Px1y[X[i],y[i]] += 1

# normalize each column by total number to convert counts into probability 
Px1y = Px1y/np.sum(Px1y,0)

# P(x(1)|y) 
data = pd.DataFrame(Px1y,columns=['P(x(1) | y = 0)','P(x(1) | y=1)'],index=['x(1) = 0','x(1) = 1','x(1) = 2'])
data

# k = 2: temperature feature 
# P(x(2) | y) = ? 

X = temp_encoded
y = label

# count the instances 
Px2y = np.zeros([3,2])
for i in range(0,len(X)):   
  Px2y[X[i],y[i]] += 1

# normalize each column by total number to convert counts into probability 
Px2y = Px2y/np.sum(Px2y,0)

# P(x(1)|y) 
data = pd.DataFrame(Px2y,columns=['P(x(2) | y = 0)','P(x(2) | y=1)'],index=['x(2) = 0','x(2) = 1','x(2) = 2'])
data

"""Now we have all the conditional probabilities to compute the posterior probability of the label under Naive Bayes classifier assumptions: 

$$P(y | [x(1)~x(2)]) \propto P(x(1)|y) P(x(2)|y) P(y)$$. 

We compute this probability for $y=0,1$ and pick the label with larger probability. 
"""

# Let us first test this probability over the training data 
# print(features[2,1])
# print(Px1y[features[2,0],0]*Px2y[features[2,0],0])

Py0x = np.zeros(len(features))
Py1x = np.zeros(len(features))
for i in range(0,len(features)):
  Py0x[i] = Px1y[features[i,0],0]*Px2y[features[i,0],0]*Py[0]
  Py1x[i] = Px1y[features[i,0],1]*Px2y[features[i,0],0]*Py[1]

# normalize to probability 
sum_Py0x_Py1x = Py0x+Py1x
Py0x = Py0x/sum_Py0x_Py1x
Py1x = Py1x/sum_Py0x_Py1x

# P(y|[x(1) x(2)]) 
data = pd.DataFrame({'P(y = 0| x )':Py0x,'P(y = 1 | x)':Py1x})
data

"""### Generating Model
Generate a model using naive bayes classifier in the following steps:

* Create naive bayes classifier
* Fit the dataset on classifier
* Perform prediction

We will probably see the following variant of Naive Bayes called Gaussian Naive Bayes later in the course when we talk about Gaussian discriminant analysis. 

[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:

$$P(x(i) | y) = \frac{1}{\sqrt{2\pi \sigma_y^2}}\exp\left(-\frac{(x(i)-\mu_y)^2}{2\sigma_y^2}\right)$$   
 
The parameters $\mu_y, \sigma_y$ are estimated using maximum likelihood.
"""

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
model = GaussianNB()

# Train the model using the training sets
model.fit(features,label)


#Predict Output on training data
predicted= model.predict(features) 
print("Predicted Value:", predicted)

#Predict Output on a test sample
predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild
print("Predicted Value:", predicted)

"""Here, 1 indicates that players can 'play'.


"""

#Import Multinomial Naive Bayes model
from sklearn.naive_bayes import MultinomialNB

#Create a MultinomialNB Classifier
model = MultinomialNB(alpha =0.00000001)

# Train the model using the training sets
model.fit(features,label)


#Predict Output on training data
predicted= model.predict(features) 
print("Predicted Value:", predicted)

#Predict Output on a test sample
predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild
print("Predicted Value:", predicted)

"""### Zero probability problem

If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called [pseudocount](https://en.wikipedia.org/wiki/Pseudocount), in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called [Laplace](https://en.wikipedia.org/wiki/Laplace_smoothing) smoothing when the pseudocount is one, and [Lidstone](https://en.wikipedia.org/wiki/Lidstone_smoothing) smoothing in the general case.

## Naive Bayes with Multiple Labels
Till now you have learned Naive Bayes classification with binary labels. Now you will learn about multiple class classification in Naive Bayes. Which is known as multinomial Naive Bayes classification. For example, if you want to classify a news article about technology, entertainment, politics, or sports.

In model building part, you can use wine dataset which is a very famous multi-class classification problem. "This dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars." ([UC Irvine](https://archive.ics.uci.edu/ml/datasets/wine))

Dataset comprises of 13 features (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) and type of wine cultivar. This data has three type of wine Class_0, Class_1, and Class_3. Here you can build a model to classify the type of wine.

The dataset is available in the scikit-learn library.

### Loading Data
Let's first load the required wine dataset from scikit-learn datasets.
"""

#Import scikit-learn dataset library
from sklearn import datasets

#Load dataset
wine = datasets.load_wine()

"""### Exploring Data
You can print the target and feature names, to make sure you have the right dataset, as such:
"""

# print the names of the 13 features
print("Features: ", wine.feature_names)

# print the label type of wine(class_0, class_1, class_2)
print("Labels: ", wine.target_names)

"""It's a good idea to always explore your data a bit, so you know what you're working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset.


"""

# print data(feature)shape
print(wine.data.shape)

# print the wine data features (top 5 records)
print("Data:",wine.data[0:5])

# print the wine labels (0:Class_0, 1:class_2, 2:class_2)
print("Target:",wine.target)

"""### Splitting Data

First, you separate the columns into dependent and independent variables (or features and label). Then you split those variables into train and test set.
"""

# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109) # 70% training and 30% test

"""### Model Generation

After splitting, you will generate a naive Bayes model on the training set and perform prediction on test set features.


"""

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = gnb.predict(X_test)

"""### Evaluating Model

After model generation, check the accuracy using actual and predicted values.
"""

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))